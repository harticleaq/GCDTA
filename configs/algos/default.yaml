# This is the configuration file

# whether to use the specified seed
seed_specify: True
# seed
seed: 123

# whether to use CUDA
device: cuda:0
# whether to set CUDA deterministic
cuda_deterministic: False
# arg to torch.set_num_threads
torch_threads: 8

# total training epochs
total_epochs: 20
# logging interval (currently unused)
log_interval: ~
# whether to use ValueNorm
use_linear_lr_decay: True
# whether to consider the case of truncation when an episode is done
model_dir: /home/aqh/haq_pro/bio_information/new/result/4
# whether to use evaluation
use_eval: True
# weight regular
weight_decay: 0.0001
# dropout
dropout: 0.2

# network parameters
# def base decoder arch of the model: transfomer, crosstransformer
base_decoder: transformer
# define seq network type: cnn, dilated
seq_net_type: dilated
# first norm or next norm
first_norm: False
# final activation function, choose from sigmoid, gelu, tanh, relu, leaky_relu, selu
final_activation_func: leaky_relu
# initialization method for network parameters, choose from xavier_uniform_, orthogonal_, ...
initialization_method: xavier_uniform_
# gain of the gat weight layer of the network.
gain: 1.414
# optimizer parameters
lr: 0.0003
# define the loss function of model
loss_function: mse

# trainging batch size
batch_size: 32
# smiles embedding size in GAT
smi_hidden_size: 128
# seq embedding size in Dilated CNN
seq_hidden_size: 128
# define the max seq sequence length
max_seq_len: 1000
# define the max smile sequence length
max_smi_len: 150
# define the initial seq feature dim
seq_size: 40
# define the initial smile feature dim
smi_size: 64
# define the cross attention dim
att_dim: 128
# define the number of layers in cross attention
n_layers: 2
# define the number of attention heads in cross attention
n_heads: 8

# logging directory
log_dir: ~
