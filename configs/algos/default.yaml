# This is the configuration file

# whether to use the specified seed
seed_specify: True
# seed
seed: 777

# whether to use CUDA
device: cuda:0
# whether to set CUDA deterministic
cuda_deterministic: False
# arg to torch.set_num_threads
torch_threads: 8

total_epochs: 30
# logging interval (currently unused)
log_interval: ~
# whether to use ValueNorm
use_valuenorm: False
# whether to use linear learning rate decay
use_linear_lr_decay: False
# whether to consider the case of truncation when an episode is done
use_proper_time_limits: True
# if set, load models from this directory; otherwise, randomly initialise the models
model_dir: ~

# whether to use evaluation
use_eval: True
# number of parallel environments for evaluation
n_eval_rollout_threads: 1
# number of episodes per evaluation
eval_episodes: 10


# network parameters
# hidden sizes for mlp module in the network
hidden_sizes: [256, 256]
# activation function, choose from sigmoid, tanh, relu, leaky_relu, selu
activation_func: relu
# whether to use feature normalization
use_feature_normalization: True
# final activation function, choose from sigmoid, tanh, relu, leaky_relu, selu
final_activation_func: tanh
# initialization method for network parameters, choose from xavier_uniform_, orthogonal_, ...
initialization_method: orthogonal_
# gain of the output layer of the network.
gain: 0.01
# optimizer parameters
# actor learning rate
lr: 0.0003

batch_size: 16
# coefficient for target model soft update
polyak: 0.005
# the number of steps to look ahead
n_step: 5
# whether to use huber loss
use_huber_loss: True
# whether to use policy active masks
use_policy_active_masks: True
# huber delta
huber_delta: 10.0
# whether to share parameter among actors
share_param: False
# whether to use a fixed optimisation order
fixed_order: False
# whether to use motivational communication
use_comm: True
# the mi loss weight
mi_loss_weight: 0.001

smi_hidden_size: 256
seq_hidden_size: 256
max_seq_len: 1000
max_smi_len: 78
seq_size: 54
smi_size: 64
att_dim: 64

n_layers: 2
n_heads: 4


# logging directory
log_dir: ~
